{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ab0651",
   "metadata": {},
   "source": [
    "Ensemble techniques\n",
    "1. bagging\n",
    "2. boosting\n",
    "\n",
    "## **Random Forest**\n",
    "\n",
    "basic intution\n",
    "\n",
    "Bagging techniques\n",
    "We have a dataset and we have a set of n models ( or base learners - could be logistic tree, naive bayes, decision trees) and each of them will give their output and we take the majority voting of those outputs to decide which the given new test data belongs to. this is for classification problems. in case of regression problems, we take the average of these outputs instead of majority voting to decide which class they belong to.\n",
    "- all these models (or base learners) get trained **parallely** not sequentially\n",
    "- solves both classification and regression problems\n",
    "- ex: random forest\n",
    "\n",
    "\n",
    "Boosting\n",
    "\n",
    "- in boosting we have weak learners\n",
    "- weak learners are connected **sequentially**\n",
    "- we combine multiple weak learners to give a strong prediction\n",
    "- each weak learner leanrns and predicts from the dataset and the parts which it is not able to predict properly sends to the next model which is sequentially connected along with some more dataset.\n",
    "- in short each model corrects the mistakes of the previous model to give more correct prediction in the next levels\n",
    "\n",
    "ex: adaboost, xgboost(xtreme gradient boost), gradient boost\n",
    "\n",
    "\n",
    "\n",
    "why random forest over decision trees when it also uses decision trees?\n",
    "- decision trees have very high time complexity for regression or classification tasks\n",
    "- but in random forest each of the base learners (smaller models of decision trees) are good at something and make sure that the data is split among all the models and they have a very high test accuracy.\n",
    "- decision trees have low bias and high variance and sometimes lead to over fitting\n",
    "- since random forest has low bias and also reduces the high variance to low variance and also takes into account the majority voting we can say that the Random forest is a better choice over the Decision trees\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27301a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
